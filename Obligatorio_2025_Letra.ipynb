{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obligatorio - Taller Agentes Inteligentes 2025\n",
    "\n",
    "En este trabajo obligatorio aplicaremos los conceptos vistos en el curso para dise√±ar, implementar y evaluar agentes capaces de aprender a jugar al cl√°sico **Breakout** de Atari, utilizando el entorno provisto por Farama Gymnasium ([https://ale.farama.org/environments/breakout/](https://ale.farama.org/environments/breakout/)). \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.tenor.com/oMxHgRrISJsAAAAM/atari-deep-learning.gif\" alt=\"Atari Deep Learning\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "El ejercicio se enmarca en un contexto de aprendizaje pr√°ctico, donde trabajaremos con las interfaces est√°ndar de Gymnasium para:\n",
    "\n",
    "1. **Profundizar en algoritmos de valor**: implementaremos y compararemos dos variantes de Q-Learning basadas en redes neuronales profundas:\n",
    "   * **Deep Q Learning (DQN)**\n",
    "   * **Double Deep Q Learning (DDQN)**\n",
    "2. **Evaluar rendimiento y estabilidad**: registraremos las recompensas obtenidas durante el entrenamiento de cada agente y analizaremos su comportamiento mediante gr√°ficas comparativas.\n",
    "3. **Demostrar resultados de forma visual**: capturaremos v√≠deos que muestren a cada agente ‚Äúresolviendo‚Äù el entorno, entendido como la habilidad de romper al menos cinco bloques en una partida.\n",
    "\n",
    "Debido a las limitaciones de tiempo y c√≥mputo propias de un entorno de curso, no se espera entrenar modelos durante m√°s de diez horas por agente. Por ello, ser√° fundamental:\n",
    "\n",
    "* Integrar puntos de **checkpoint** para guardar peri√≥dicamente los pesos de la red.\n",
    "* Seguir en los puntos 2 y 3 la arquitectura y t√©cnicas originales propuestas en los papers seminales de DQN y DDQN, dejando la experimentaci√≥n adicional para el punto extra.\n",
    "* Flexibilizar la notebook de gu√≠a: pueden reorganizarla o dividirla en m√∫ltiples archivos seg√∫n su conveniencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos y tareas\n",
    "\n",
    "1. **Completar la implementaci√≥n**\n",
    "   * Rellenar el c√≥digo faltante en la notebook y en los m√≥dulos auxiliares para que los agentes puedan interactuar con el entorno de Breakout.\n",
    "2. **Entrenar agentes**\n",
    "   * Ajustar y entrenar un **DQN** que alcance la condici√≥n de ‚Äúresolver‚Äù (romper ‚â• 10 bloques).\n",
    "   * Ajustar y entrenar un **DDQN** con la misma meta de desempe√±o.\n",
    "3. **Visualizar y analizar resultados**\n",
    "   * Generar **gr√°ficas comparativas** de las recompensas obtenidas por ambos agentes en el mismo entorno (una gr√°fica por ambiente). Adem√°s se sugiere gr√°ficas que muestren el valor de la funci√≥n de valor Q para cada agente.\n",
    "   * Extraer **al menos dos conclusiones** por gr√°fica, comentando diferencias en convergencia, estabilidad y comportamiento exploratorio.\n",
    "4. **Registro de demostraciones**\n",
    "   * Grabar y entregar un **video demostrativo** de cada agente resolviendo el entorno.\n",
    "5. **Experimentaci√≥n**\n",
    "   * Probar otras arquitecturas, t√©cnicas de mejora o m√≥dulos de procesamiento de entradas m√°s avanzados, documentando brevemente su impacto/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterios de evaluaci√≥n\n",
    "\n",
    "| Criterio                                  | Descripci√≥n                                                  | Peso |\n",
    "| ----------------------------------------- | ------------------------------------------------------------ | ---- |\n",
    "| **Implementaci√≥n y rendimiento**          | DQN y DDQN completados; cada agente rompe ‚â• 10 bloques       | 40%  |\n",
    "| **Estructura y narrativa de la notebook** | Secciones claras, explicaci√≥n de decisiones, ‚Äúhistoria‚Äù      | 20%  |\n",
    "| **An√°lisis de resultados**                | Gr√°ficas comparativas; ‚â• 2 conclusiones por gr√°fico          | 20%  |\n",
    "| **Presentaci√≥n visual**                   | V√≠deos demostrativos de cada agente                          | 10%  |\n",
    "| **Experimentaci√≥n**                       | Experimentaci√≥n adicional documentada y analizada brevemente | 10%  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliograf√≠a\n",
    "\n",
    "* **Mnih, V.**, Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ‚Ä¶ Hassabis, D. (2013). *Playing Atari with Deep Reinforcement Learning*. [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)\n",
    "* **van Hasselt, H.**, Guez, A., & Silver, D. (2015). *Deep Reinforcement Learning with Double Q-learning*. [arXiv:1509.06461](https://arxiv.org/abs/1509.06461)\n",
    "* **Sutton, R. S.**, & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.), cap√≠tulo 16.5: ‚ÄúHuman-level Video Game Play‚Äù. MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc6t9etEt9I2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwHCw6PMt9I3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import gymnasium\n",
    "import ale_py\n",
    "from utils import make_env, show_observation_stack, plot_rewards_by_episode, plot_avg_reward_by_episode,plot_rewards_comparison\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALE (Atari Learning Environment) es un entorno de aprendizaje para videojuegos de Atari. En este caso, utilizaremos el entorno de Breakout. Es necesario entender que se separa el entorno de los roms de Atari, que son los juegos en s√≠. El entorno de ALE permite interactuar con los juegos de Atari a trav√©s de una API est√°ndar, facilitando la implementaci√≥n de algoritmos de aprendizaje por refuerzo.\n",
    "\n",
    "Debemos instalar los roms por separado, para ello primero tenemos que saber donde est√°n los roms de Atari. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gymnasium.register_envs(ale_py) # registramos todos los entornos de ale_py\n",
    "ruta_init = ale_py.roms.__file__ # debemos saber donde se encuentra la carpeta roms\n",
    "ALE_ROMS_PATH = os.path.dirname(ruta_init)\n",
    "print(ALE_ROMS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar los siguientes comando para instalar los roms y colocalos en la carpeta correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"autorom[accept-rom-license]\"\n",
    "#!AutoROM --accept-license --install-dir {ALE_ROMS_PATH}\n",
    "#Lo hice por fuera por que en la colab no funcionaba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijamos la semilla para que los resultados sean reproducibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 23\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True # https://discuss.pytorch.org/t/what-is-the-differenc-between-cudnn-deterministic-and-cudnn-benchmark/38054\n",
    "torch.backends.cudnn.benchmark=True # https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/4\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que dispositivo tenemos, si es GPU, MPS o CPU. **El uso de GPU es altamente recomendable** para acelerar el entrenamiento de los modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas constantes\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando Atari\n",
    "\n",
    "Para reproducir fielmente el enfoque de Mnih et al. (2013) y reducir la carga computacional al trabajar con im√°genes de Atari (210 √ó 160 p√≠xeles, 128 colores), aplicamos el siguiente preprocesado œÜ a las √∫ltimas **4** frames del entorno:\n",
    "\n",
    "1. **Conversi√≥n a escala de grises**\n",
    "   Eliminamos la informaci√≥n de color (GRAYSCALE = True), pues la luminosidad es suficiente para capturar la din√°mica de juego y reduce dr√°sticamente la dimensionalidad de la entrada.\n",
    "\n",
    "2. **Down-sampling y recorte**\n",
    "   * Redimensionamos la imagen original a 110 √ó 84 p√≠xeles, manteniendo la proporci√≥n horizontal.\n",
    "   * Recortamos un √°rea central de 84 √ó 84 p√≠xeles que contiene la ‚Äúzona de juego‚Äù, descartando bordes innecesarios.\n",
    "     Este paso (SCREEN_SIZE = 84) no solo concentra la atenci√≥n del modelo en la regi√≥n relevante, sino que tambi√©n garantiza un tama√±o cuadrado compatible con las implementaciones de convoluciones en GPU.\n",
    "\n",
    "3. **Saltos temporales (frame skipping)**\n",
    "   Procesamos cada 4 frames (SKIP_FRAMES = 4), repitiendo la misma acci√≥n durante esos pasos. Esto reduce la redundancia temporal, acelera el entrenamiento y mantiene la coherencia del movimiento de la paleta y la bola.\n",
    "\n",
    "4. **Apilamiento de frames**\n",
    "   Finalmente, acumulamos las √∫ltimas 4 im√°genes preprocesadas (NUM_STACKED_FRAMES = 4) en un √∫nico tensor de entrada. As√≠ el agente puede inferir la velocidad y direcci√≥n de los elementos m√≥viles a partir de la diferencia entre frames.\n",
    "\n",
    "Este esquema de preprocesado es fundamental para disminuir el espacio de entrada, acelerar las convoluciones y proporcionar al Q-net una representaci√≥n compacta y rica en informaci√≥n din√°mica, tal como se describe en el algoritmo 1 del paper original .\n",
    "\n",
    "> Se recomienda ver el m√©todo `make_env` en el archivo `utils.py` para entender c√≥mo se implementa este preprocesado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAY_SCALE = True # si True, convertimos la imagen a escala de grises\n",
    "SCREEN_SIZE = 84 # redimensionamos a SCREEN_SIZExSCREEN_SIZE\n",
    "NUM_STACKED_FRAMES = 4 # apilamos NUM_STACKED_FRAMES frames\n",
    "SKIP_FRAMES = 4 # saltamos SKIP_FRAMES frames (haciendo la misma acci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loVxQPrwt9I5",
    "outputId": "18b7ed97-88dd-4b1e-a2cc-b2636686bfc7"
   },
   "outputs": [],
   "source": [
    "# https://ale.farama.org/environments/breakout/\n",
    "ENV_NAME = \"ALE/Breakout-v5\" \n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/random',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=None,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "for episode_num in range(1):\n",
    "    obs, info = env.reset()\n",
    "    show_observation_stack(obs)\n",
    "    reward_total = 0\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # seleccionamos una acci√≥n aleatoria\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        reward_total += reward\n",
    "        episode_over = terminated or truncated\n",
    "    print(f\"Episode {episode_num + 1} finished with total reward: {reward_total}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem√°s, podemos mostrar los videos capturados por el entorno de Atari de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de v√≠deo en tu sistema de ficheros\n",
    "video_path = \"./videos/random/breakout-episode-0.mp4\"\n",
    "\n",
    "# Muestra el v√≠deo\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYVG_TKt9I5"
   },
   "source": [
    "# Exploraci√≥n del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\",env.action_space)\n",
    "print(\"Observation shape:\",env.observation_space.shape)\n",
    "env.reset()\n",
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {terminated},\\n {info}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acciones\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIHpwiaat9I7"
   },
   "source": [
    "# Deep Q Learning\n",
    "\n",
    "Deep Q Learning extiende el algoritmo cl√°sico de Q-learning al emplear una **red neuronal profunda** como aproximador de la funci√≥n de valor $Q(s,a)$. Inspirado en Mnih et al. (2013), este m√©todo utiliza una **red convolucional** para procesar directamente las im√°genes del entorno Atari, un **replay buffer** para romper la correlaci√≥n temporal de las muestras. La pol√≠tica sigue un esquema **Œµ-greedy**, balanceando exploraci√≥n y explotaci√≥n, y se entrena minimizando el error de la ecuaci√≥n de Bellman sobre lotes de transiciones muestreadas de manera aleatoria.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/profile/Faris-Mismar/publication/327045314/figure/fig4/AS:819677282455553@1572437701142/Structure-of-the-neural-network-used-for-the-Deep-Q-learning-Network-implementation-with.png\" alt=\"DQN\"/>\n",
    "</p>\n",
    "\n",
    "Fuente: [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajTGajUftSgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Memoria\n",
    "\n",
    "El algoritmo de Deep Q Learning presentado en el paper utiliza una memoria (llamada Replay Memory) para almacenar transiciones pasadas. Tuplas que contienen un estado base, la accion tomada, la recompensa obtenida, una bandera que indica si el siguiente estado es final o no; y el estado siguiente.\n",
    "\n",
    "Esta memoria es circular, es decir, tiene un l√≠mite maximo de elementos y una vez est√© llena comienza a reemplazar los elementos m√°s viejos.\n",
    "\n",
    "Vamos a necesitar crear una funci√≥n **sample** que obtiene una mustra aleatoria de elementos de la memoria.  Esto puede ser una lista de Transiciones o listas separadas (pero alineadas) de los elementos que las componen.\n",
    "\n",
    "\n",
    "> Para implementar esta funcionalidad se debe modificar el archivo **replay_memory.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scPtpbz4tTAh",
    "outputId": "0890f48a-e673-4416-bd69-7dcf2730ba64",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import replay_memory \n",
    "from replay_memory import ReplayMemory, Transition\n",
    "import importlib\n",
    "importlib.reload(replay_memory)\n",
    "\n",
    "# Creamos la memoria de repetici√≥n\n",
    "replay_memory = ReplayMemory(3, lambda obs,device:torch.tensor(obs, dtype=torch.float32, device=device) / 255.0)\n",
    "\n",
    "# A√±adimos transiciones a la memoria (solo las 3 √∫ltimas se guardan)\n",
    "replay_memory.add(np.ones((4,84,84)), 0, 10, False, np.ones((4,84,84)))\n",
    "replay_memory.add(np.ones((4,84,84))*2, 1, 10, False, np.ones((4,84,84))*2)\n",
    "replay_memory.add(np.ones((4,84,84))*3, 2, 10, False, np.ones((4,84,84))*3)\n",
    "replay_memory.add(np.ones((4,84,84))*4, 3, 10, True, np.ones((4,84,84))*4)\n",
    "\n",
    "# Mostramos el tama√±o de la memoria\n",
    "print(f\"Memory size: {len(replay_memory)}\\n\")\n",
    "\n",
    "# Sampleamos 3 transiciones de la memoria\n",
    "sampled_transition = replay_memory.sample(3)\n",
    "\n",
    "# Comprobamos los shapes de los datos\n",
    "(state, action, reward, done, next_state) = sampled_transition[0]\n",
    "print(\"Sate shape:\", state.shape)\n",
    "print(\"Action shape:\", action.shape)\n",
    "print(\"Reward shape:\", reward.shape)\n",
    "print(\"Done shape:\", done.shape)\n",
    "print(\"Next state shape:\", next_state.shape)\n",
    "\n",
    "# Mostramos un sample de la memoria\n",
    "print(f\"Memory sample:\")\n",
    "for i, sample in enumerate(sampled_transition):\n",
    "    print(f\"Sample {i}: {sample}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7Ygv5Mjtb-F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelo\n",
    "\n",
    "Vamos a usar un mismo modelo FeedForward para estos dos problemas (entrenado en cada problema particular). Recomendamos simplicidad en la creaci√≥n del mismo, pero tienen total libertad al momento de implementarlo.\n",
    "\n",
    "> Para implementar esta funcionalidad se debe modificar el archivo **dqn_cnn_model.py**. Se recomienda empezar por una arquitectura simple como la que se muestra en el paper de Mnih et al. (2013) y luego experimentar con arquitecturas m√°s complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkNBvJB6ryp7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dqn_cnn_model import DQN_CNN_Model\n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                record_every=None,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "print(\"Actions shape:\",env.action_space)\n",
    "print(\"Observatiion shape:\",env.observation_space.shape)\n",
    "\n",
    "env.close()\n",
    "\n",
    "cnn_model = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "summary(cnn_model, input_size=(32, SKIP_FRAMES, SCREEN_SIZE, SCREEN_SIZE), device=DEVICE) # 32 es el batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red esta definida para que reciba un tensor de 4 dimensiones (batch_size, num_frames, height, width) y devuelve un tensor de 2 dimensiones (batch_size, num_actions). La funci√≥n `forward` es la encargada de definir el flujo de datos a trav√©s de la red. En este caso, se utiliza una red convolucional seguida de capas totalmente conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tensor_batch = torch.rand((NUM_STACKED_FRAMES,SCREEN_SIZE,SCREEN_SIZE)).unsqueeze(0).to(DEVICE) # A√±adimos una dimensi√≥n para el batch y lo pasamos al dispositivo\n",
    "print(f\"Q-values shape: {cnn_model(obs_tensor_batch).shape}\") # shape: (1, num_actions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de Q se obtienen a partir de la salida de la red, cada columna representa el valor Q para cada acci√≥n posible en el estado actual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model(obs_tensor_batch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos preguntar cu√°l es la acci√≥n con mayor valor Q en un estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model(obs_tensor_batch).max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejemplo vamos a ver c√≥mo tomar los valores de acciones deseables para un conjunto de estados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos 3 observaciones aleatorias para probar el modelo\n",
    "obs_ran1 = torch.rand(4, 84, 84)\n",
    "obs_ran2 = torch.rand(4, 84, 84)\n",
    "obs_ran3 = torch.rand(4, 84, 84)\n",
    "\n",
    "batch = torch.stack([obs_ran1, obs_ran2, obs_ran3], dim=0).to(DEVICE) # shape: (3, 4, 84, 84)\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "\n",
    "actions =  torch.tensor([1, 2, 3], device=DEVICE).unsqueeze(1) # queremos la acci√≥n 1 para la primera observaci√≥n, la acci√≥n 2 para la segunda y la acci√≥n 3 para la tercera\n",
    "\n",
    "Q_test = cnn_model(batch)\n",
    "print(f\"Q-values: {Q_test}\")\n",
    "print(f\"Q-values: {Q_test.gather(1, actions)}\") # https://pytorch.org/docs/main/generated/torch.gather.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi \n",
    "\n",
    "La funci√≥n para procesar los estados (phi en el paper) que es necesaria para poder usar el modelo de Pytorch con las representaciones de gym. Esta funci√≥n pasa una observaci√≥n de gym a un tensor de Pytorch y la normaliza.\n",
    "\n",
    "> T√©cnicamente la funci√≥n phi tiene m√°s responsabilidades, como la de apilar los frames y el downsampling. En nuestro caso se lo delegamos a los wrappers de gymnasium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oegpMg25t9I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_state(obs, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Preprocess the state to be used as input for the model (transform to tensor).\n",
    "    \"\"\"\n",
    "    return torch.tensor(obs, dtype=torch.float32, device=device) / 255.0\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = process_state(obs)\n",
    "print(f\"Observation shape: {obs_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9B7ZY9Htj_F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Agente\n",
    "\n",
    "Vamos a definir una clase agente (abstracto), encargado de interactuar con el ambiente y entrenar los modelos. Los m√©tdos definidos deben funcionar para ambos problemas simplemente cambiando el modelo a utilizar para cada ambiente.\n",
    "\n",
    "Abajo dejamos un esqueleto del mismo y las funciones a completar. Recomendamos no alterar la estructura del mismo, pero pueden definir las funciones auxiliares que consideren necesarias.\n",
    "\n",
    "> Para implementar esta funcionalidad se debe modificar los archivos **abstract_agent.py** y **dqn_agent.py**.\n",
    "\n",
    "Funciones a completar:\n",
    "\n",
    "\n",
    "1. **init**: que inicializa los parametros del agente.\n",
    "\n",
    "2. **compute_epsilon**: que computa el valor actual de epsilon en base al n√∫mero de pasos actuales y si esta entrenando o no.\n",
    "\n",
    "3. **select_action**: Seleccionando acciones \"epsilongreedy-mente\" si estamos entranando y completamente greedy en otro caso.\n",
    "\n",
    "4. **train**: que entrena el agente por un n√∫mero dado de episodios de largo determinado.\n",
    "\n",
    "5. **record_test_episode**: para grabar un episodio con el agente siempre seleccionando la mejor accion conocida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOD-ENZRtyMt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperpar√°metros de entrenamiento del agente DQN\n",
    "TOTAL_STEPS = 10_000_000\n",
    "EPISODES = 10_000\n",
    "STEPS_PER_EPISODE = 20_000\n",
    "\n",
    "EPSILON_INI = 1\n",
    "EPSILON_MIN = 0.05\n",
    "EPSILON_ANNEAL_STEPS = 1_000_000\n",
    "\n",
    "EPISODE_BLOCK = 100\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50_000\n",
    "\n",
    "GAMMA = 0.995\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dqn_agent\n",
    "from dqn_agent import DQNAgent\n",
    "importlib.reload(dqn_agent)\n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/dqn_training',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=500,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "net = DQN_CNN_Model(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "dqn_agent = DQNAgent(env, net, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA, epsilon_i=EPSILON_INI, epsilon_f=EPSILON_MIN, epsilon_anneal_steps=EPSILON_ANNEAL_STEPS, episode_block=EPISODE_BLOCK, device=DEVICE)\n",
    "\n",
    "dqn_rewards = dqn_agent.train(EPISODES, STEPS_PER_EPISODE, TOTAL_STEPS)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/dqn_validation',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=10,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "dqn_agent.play(env, episodes=3)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de v√≠deo en tu sistema de ficheros\n",
    "video_path = \"./videos/dqn_validation/breakout-episode-0.mp4\"\n",
    "\n",
    "# Muestra el v√≠deo\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_reward_by_episode(dqn_rewards, average_range=100)\n",
    "plot_rewards_by_episode(dqn_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QETh1K7pt9I_"
   },
   "source": [
    "# Double Deep Q Learning\n",
    "\n",
    "Double DQN mejora la versi√≥n cl√°sica de DQN corrigiendo el sesgo de sobreestimaci√≥n de los valores \n",
    "ùëÑ\n",
    "Q. Para ello, desacopla la selecci√≥n de la acci√≥n de su evaluaci√≥n: en cada paso, la red online elige la acci√≥n que maximiza \n",
    "ùëÑ\n",
    "Q, pero la red objetivo distinta estima el valor de esa acci√≥n. Esta separaci√≥n reduce el sesgo hacia valores demasiado optimistas y aporta mayor estabilidad al entrenamiento. El resto de la estructura ‚Äîreplay buffer, pol√≠tica Œµ-greedy, etc‚Äî se mantiene igual que en DQN, aprovechando as√≠ un dise√±o casi id√©ntico al original pero con resultados m√°s fiables .\n",
    "\n",
    "Fuente: [arXiv:1509.06461](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "> Vamos a utilizar el mismo modelo de red neuronal creado para el problema anterior y la misma implementaci√≥n de memoria, dejamos un esqueleto de un agente de Double Deep Q learning para completar en el archivo **double_dqn_agent.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDNkAtdMt9I_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import double_dqn_agent\n",
    "from double_dqn_agent import DoubleDQNAgent\n",
    "importlib.reload(double_dqn_agent)\n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/ddqn_training',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=500,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES)\n",
    "\n",
    "\n",
    "modelo_a = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "modelo_b = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "\n",
    "ddqn_agent = DoubleDQNAgent(env, modelo_a, modelo_b, process_state, BUFFER_SIZE, BATCH_SIZE, LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI, epsilon_f=EPSILON_MIN, epsilon_anneal_steps=EPSILON_ANNEAL_STEPS, episode_block = EPISODE_BLOCK, device=DEVICE)\n",
    "\n",
    "ddqn_rewards = ddqn_agent.train(EPISODES, STEPS_PER_EPISODE, TOTAL_STEPS)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/ddqn_validation',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=1,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES\n",
    "                )\n",
    "\n",
    "ddqn_agent.play(env, episodes=3)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de v√≠deo en tu sistema de ficheros\n",
    "video_path = \"./videos/ddqn_validation/breakout-episode-0.mp4\"\n",
    "\n",
    "# Muestra el v√≠deo\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_reward_by_episode(ddqn_rewards, average_range=100)\n",
    "plot_rewards_by_episode(ddqn_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentaci√≥n\n",
    "\n",
    "## Doble DQN con prioritised Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prioritised_double_dqn_agent\n",
    "from prioritised_double_dqn_agent import PrioritisedDoubleDQNAgent\n",
    "importlib.reload(prioritised_double_dqn_agent)\n",
    "\n",
    "env = make_env(ENV_NAME,\n",
    "                video_folder='./videos/prioritised_ddqn_training',\n",
    "                name_prefix=\"breakout\",\n",
    "                record_every=500,\n",
    "                grayscale=GRAY_SCALE,\n",
    "                screen_size=SCREEN_SIZE,\n",
    "                stack_frames=NUM_STACKED_FRAMES,\n",
    "                skip_frames=SKIP_FRAMES)\n",
    "\n",
    "\n",
    "online_model = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "target_model = DQN_CNN_Model(env.observation_space.shape, env.action_space.n).to(DEVICE)\n",
    "prioritised_ddqn_agent = PrioritisedDoubleDQNAgent(gym_env=env, \n",
    "                                       model_a=online_model, \n",
    "                                       model_b=target_model, \n",
    "                                       obs_processing_func=process_state, \n",
    "                                       memory_buffer_size=BUFFER_SIZE, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       learning_rate=LEARNING_RATE, \n",
    "                                       gamma=GAMMA, \n",
    "                                       epsilon_i=EPSILON_INI, \n",
    "                                       epsilon_f=EPSILON_MIN, \n",
    "                                       epsilon_anneal_steps=EPSILON_ANNEAL_STEPS, \n",
    "                                       episode_block = EPISODE_BLOCK, \n",
    "                                       device=DEVICE)\n",
    "\n",
    "prioritised_ddqn_rewards = prioritised_ddqn_agent.train(EPISODES, STEPS_PER_EPISODE, TOTAL_STEPS)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_reward_by_episode(prioritised_ddqn_rewards, average_range=100)\n",
    "plot_rewards_by_episode(prioritised_ddqn_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNNvsKiEt9I_"
   },
   "source": [
    "# Discusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards_comparison(dqn_rewards, ddqn_rewards,prioritised_ddqn_rewards, labels=[\"DQN\", \"DDQN\", \"Prioritised DDQN\"], average_range=100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "obl_taller_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
